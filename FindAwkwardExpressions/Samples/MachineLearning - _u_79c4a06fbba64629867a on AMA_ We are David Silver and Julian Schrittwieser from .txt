/u/79c4a06fbba64629867a on AMA: We are David Silver and Julian Schrittwieser from DeepMindâ€™s AlphaGo team. Ask us anything.
It seems that training by self-play entirely would have been the first thing you would try in this situation before trying to scrape together human game data. What was the reason that earlier versions of AlphaGo didn't train through self-play or if it was attempted, why didn't it work as well? In general, I am curious about how development and progress works in this field. What would have been the bottleneck two years ago in designing a self-play trained AlphaGo compared to today? What "machine learning intuition" was gained from all the iterations that finally made a self-play system viable?   