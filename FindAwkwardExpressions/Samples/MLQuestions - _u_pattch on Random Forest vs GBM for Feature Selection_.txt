/u/pattch on Random Forest vs GBM for Feature Selection?
If both approaches work similarly well but use different features, then it may be best to use the approach that uses the fewer number of features. If you can successfully accomplish your task with a smaller number of features, the extra features you would add in allows for more over fitting. I don’t know if there’s any general rule about RF vs GBM, I am personally a fan of RFs and haven’t used GBMs much so take what I say with a grain of salt. One last note is that you could take a look at the actual features each approach takes advantage of, and ask if those features make sense as being predictive of your classes. If there’s no real causal link to one set of features, then it’s likely that the model using those features is also overfitting. This kind of analysis only works if you have some domain knowledge for the task, but can be helpful when you can apply it   